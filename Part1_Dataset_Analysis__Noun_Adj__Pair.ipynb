{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1 - Dataset Analysis - <Noun - Adj> Pair",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPdV44bD_Qp7",
        "outputId": "db5820b0-c090-42fb-d810-70c39a8e8b7f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0VEJjNcDihb"
      },
      "source": [
        "#### import data and libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksKeEgyH_G2Q"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import Tree\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "from nltk import ne_chunk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vcZhFurC_RH6",
        "outputId": "5ec8a3c8-a563-4710-ad37-21d4518dd5fd"
      },
      "source": [
        "with open('/content/drive/MyDrive/Y3S1/CZ4045 Natural language processing/reviewSelected100.json', encoding = \"ISO-8859-1\") as f:\n",
        "    data = f.readlines()\n",
        "data = [json.loads(line) for line in data] #convert string to dict format\n",
        "data = pd.DataFrame(data)\n",
        "data = data[['business_id', 'stars', 'text']]\n",
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
              "      <td>5.0</td>\n",
              "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Was recommended to try this place by few peopl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Ambience: Would not expect something this nice...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Absolutely the WORST pool company that I have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15295</th>\n",
              "      <td>shIPnFoXrL3dFo5HLH1_HA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>This was the worst experience ever. So much so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15296</th>\n",
              "      <td>zPEYgVqJ2QNKi45FJi2jvg</td>\n",
              "      <td>5.0</td>\n",
              "      <td>We come here every time we hit Vegas! A giant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15297</th>\n",
              "      <td>zPEYgVqJ2QNKi45FJi2jvg</td>\n",
              "      <td>1.0</td>\n",
              "      <td>As locals we used  to the this place when it w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15298</th>\n",
              "      <td>etzDsNjkCyQBoJcU2a3U-g</td>\n",
              "      <td>5.0</td>\n",
              "      <td>The food was delicious. We were seated in 15 m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15299</th>\n",
              "      <td>9Xm2GfG8Rnbb1_CmXyrm3g</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Wonderful spot (the patio) and OH SO EXCELLENT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15300 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  business_id  ...                                               text\n",
              "0      ZBE-H_aUlicix_9vUGQPIQ  ...  We had my Mother's Birthday Party here on 10/2...\n",
              "1      e-YnECeZNt8ngm0tu4X9mQ  ...  Good Korean grill near Eaton Centre. The marin...\n",
              "2      j7HO1YeMQGYo3KibMXZ5vg  ...  Was recommended to try this place by few peopl...\n",
              "3      7e3PZzUpG5FYOTGt3O3ePA  ...  Ambience: Would not expect something this nice...\n",
              "4      vuHzLZ7nAeT-EiecOkS5Og  ...  Absolutely the WORST pool company that I have ...\n",
              "...                       ...  ...                                                ...\n",
              "15295  shIPnFoXrL3dFo5HLH1_HA  ...  This was the worst experience ever. So much so...\n",
              "15296  zPEYgVqJ2QNKi45FJi2jvg  ...  We come here every time we hit Vegas! A giant ...\n",
              "15297  zPEYgVqJ2QNKi45FJi2jvg  ...  As locals we used  to the this place when it w...\n",
              "15298  etzDsNjkCyQBoJcU2a3U-g  ...  The food was delicious. We were seated in 15 m...\n",
              "15299  9Xm2GfG8Rnbb1_CmXyrm3g  ...  Wonderful spot (the patio) and OH SO EXCELLENT...\n",
              "\n",
              "[15300 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9R1qzC1Dmgp"
      },
      "source": [
        "#### Generate Random reviews and tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1a3c3t8BLQw"
      },
      "source": [
        "# Get random reviews from 50 business id and do POS tagging, return a list of tagged text\n",
        "def get_random(rating):\n",
        "  seed = 85\n",
        "  sample_size = 50\n",
        "  # Randomly select 1 review from 50 distinct business id with rating 1\n",
        "  rating_1 = data[data['stars'] == rating]\n",
        "  rating_1 = rating_1.groupby('business_id').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
        "  # Convert all text to lowercase\n",
        "  # lower_rating_1 = rating_1['text'].str.lower()\n",
        "  random_rating_1 = rating_1.sample(sample_size, random_state = seed)['text'].reset_index(drop=True)\n",
        "  tagged_low_rating = random_rating_1.str.split().map(pos_tag)\n",
        "  return list(tagged_low_rating)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUVCu-1z6PzU"
      },
      "source": [
        "NN\tnoun, singular (cat, tree)\n",
        "\n",
        "NNS\tnoun plural (desks)\n",
        "\n",
        "NNP\tproper noun, singular (sarah)\n",
        "\n",
        "JJ\tThis NLTK POS Tag is an adjective (large)\n",
        "\n",
        "JJR\tadjective, comparative (larger)\n",
        "\n",
        "JJS\tadjective, superlative (largest)\n",
        "\n",
        "Reference used: \n",
        "\n",
        "https://www.learntek.org/blog/named-entity-recognition-with-nltk/\n",
        "\n",
        "https://www.guru99.com/pos-tagging-chunking-nltk.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K6xqrBeDwS6"
      },
      "source": [
        "#### Find most frequent <noun - adj> pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDlkUFLDE5y8"
      },
      "source": [
        "from collections import Counter\n",
        "def get_n_adj_pairs(tagged_sentences):\n",
        "    noun_list = []\n",
        "    adj_list = []\n",
        "    grammar = r\"\"\"\n",
        "    CHUNK1:\n",
        "        {<NN.*><.*>?<JJ.*>}  # Any noun end with any Adjective, eg. cream is melting, service extremely slow, place poorly managed\n",
        "    \n",
        "    CHUNK2:\n",
        "        {<JJ.*><.*>?<NN.*>}  # Nouns or adjectives, end with Nouns eg. perfect time, particular person\n",
        "\n",
        "    CHUNK3: \n",
        "        {<NN.*><NN.*>}   # Noun as Adjectives, terminated with Nouns eg. school trip\n",
        "\n",
        "    \"\"\"\n",
        "    cp = RegexpParser(grammar)\n",
        "    for sentence in tagged_sentences:\n",
        "        tree = cp.parse(sentence)\n",
        "        for subtree in tree.subtrees(filter = lambda x: x.label() in ['CHUNK1', 'CHUNK2']):\n",
        "          if (str(subtree).find('NN') > 0 or str(subtree).find('NNS') > 0 or str(subtree).find('NNP') > 0) and (str(subtree).find('JJ')> 0 or str(subtree).find('JJS')> 0 or str(subtree).find('JJR')> 0):\n",
        "              nouns = [word for word, tag in subtree.leaves() if tag in ['NN', 'NNS', 'NNP']]\n",
        "              adjss = [word for word, tag in subtree.leaves() if tag in ['JJ','JJR','JJS']]\n",
        "              noun_list.extend([nouns])\n",
        "              adj_list.extend([adjss])\n",
        "    pair_list = [(m[0]+\": \"+n[0]) for m,n in zip(noun_list, adj_list)]\n",
        "    c = Counter(pair_list)\n",
        "    print(c.most_common(10))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vNidV7iE51Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab95039d-d529-4be0-df57-702d25972973"
      },
      "source": [
        "for i in range(1, 6):\n",
        "  print('For rating = ', i, 'top 10 most common pairs are')\n",
        "  get_n_adj_pairs(get_random(i))\n",
        "  print()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For rating =  1 top 10 most common pairs are\n",
            "[('time: first', 5), ('day: next', 2), ('review,: negative', 2), ('taste: bad', 2), ('side: west', 2), ('cheese: big', 2), ('review: last', 2), ('thing: only', 2), ('place: same', 1), ('years: several', 1)]\n",
            "\n",
            "For rating =  2 top 10 most common pairs are\n",
            "[('time: second', 3), ('bit: little', 3), ('time: long', 2), ('time: first', 2), ('places: many', 2), ('time: next', 2), ('service: bad', 2), (\"time: she'll\", 2), ('time: last', 2), ('something: different', 1)]\n",
            "\n",
            "For rating =  3 top 10 most common pairs are\n",
            "[('amount: little', 3), ('service: good', 3), ('nicer: much', 2), ('dogs: other', 2), ('years: few', 2), ('bit: little', 2), ('anything: special', 2), ('food: Japanese', 2), (\"didn't: much\", 2), ('Foodland: former', 1)]\n",
            "\n",
            "For rating =  4 top 10 most common pairs are\n",
            "[('day,: next', 4), ('food: good', 3), ('time: first', 2), ('light: little', 2), ('strawberry: fresh', 2), ('Service: quick', 1), ('ambience: quiet', 1), ('key!: low', 1), ('brunch: quiet', 1), ('Brisket: ok,', 1)]\n",
            "\n",
            "For rating =  5 top 10 most common pairs are\n",
            "[('hour: happy', 3), ('weekend: last', 2), ('times: many', 2), ('service.: better', 1), ('Basilica: quick', 1), ('map: small', 1), ('GREAT: top', 1), ('Pancakes: nice', 1), (\"diner'esque: perfect\", 1), ('Luckily: day.', 1)]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}